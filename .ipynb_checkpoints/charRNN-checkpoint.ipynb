{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "with open(\"shakespeare.txt\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "data = data[:1000000]\n",
    "chars = list(set(data))\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seqs(data: str, seq_length: int):\n",
    "    num_seq = (len(data) - 1) // seq_length\n",
    "\n",
    "    for i in range(num_seq):\n",
    "        begin = i * seq_length\n",
    "        in_seq = data[begin:begin + seq_length]\n",
    "        out_seq = data[begin + 1:begin + seq_length + 1]\n",
    "\n",
    "        # pre-processing step for input seq\n",
    "        int_in_seq = [char2idx[ch] for ch in in_seq]\n",
    "        X = torch.tensor(int_in_seq).unsqueeze(0).long()\n",
    "\n",
    "        # pre-processing step for target seq\n",
    "        int_out_seq = np.array([char2idx[ch] for ch in out_seq])\n",
    "        y_tg = torch.from_numpy(int_out_seq)\n",
    "        yield (X, y_tg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2, bias=True, batch_first=True, rnn_type=\"LSTM\"):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layesr = num_layers\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=hidden_size)\n",
    "    \n",
    "        if rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size,\n",
    "                               num_layers=num_layers, bias=bias, batch_first=batch_first)\n",
    "        elif rnn_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, \n",
    "                              num_layers=num_layers, bias=bias, batch_first=batch_first)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(input_size=hidden_size, hidden_size=hidden_size, \n",
    "                              num_layers=num_layers, bias=bias, batch_first=batch_first)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        out = self.embedding(input)\n",
    "        out, hidden = self.rnn(out, hidden)\n",
    "        out = out.reshape(-1, self.hidden_size)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_seq = 100\n",
    "input_size = len(chars)\n",
    "hidden_size = 128\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hidden(rnn_type: str):\n",
    "    if rnn_type == \"LSTM\":\n",
    "        return (torch.zeros((num_layers, 1, hidden_size)).cuda(), \n",
    "                torch.zeros((num_layers, 1, hidden_size)).cuda())\n",
    "    return torch.zeros((num_layers, 1, hidden_size)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach_hidden(hidden: torch.tensor, rnn_type: str):\n",
    "    if rnn_type == \"LSTM\":\n",
    "        return (hidden[0].detach(), hidden[1].detach())\n",
    "    return hidden.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs: int = 20, rnn_type: str = \"LSTM\"):\n",
    "    net = CharRNN(input_size, hidden_size, rnn_type=rnn_type).cuda().train()\n",
    "    print(net)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    running_loss = None\n",
    "    seqs = list(get_seqs(data, len_seq))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        hidden = init_hidden(rnn_type=rnn_type)\n",
    "\n",
    "        for idx, (X, y_tg) in enumerate(seqs):\n",
    "            X = X.cuda()\n",
    "            y_tg = y_tg.cuda()\n",
    "\n",
    "            # pass through network\n",
    "            Y, hidden = net(X, hidden)\n",
    "\n",
    "            # compute loss\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(Y, y_tg)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            hidden = detach_hidden(hidden=hidden, rnn_type=rnn_type)\n",
    "\n",
    "            running_loss = loss.item() if running_loss is None else running_loss * 0.99 + 0.01 * loss.item()\n",
    "\n",
    "            if idx % 1000 == 0:\n",
    "                print(\"Epoch: %d, Iter: %d, Loss: %.3f\" % (epoch, idx, running_loss))\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, rnn_type: str = \"LSTM\", len_seq: int = 1000, k: int = 5):\n",
    "    net.eval()\n",
    "\n",
    "    ch = np.random.randint(0, len(chars))\n",
    "    hidden = init_hidden(rnn_type=rnn_type)\n",
    "    sentence = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, len_seq):\n",
    "            sentence.append(ch)\n",
    "            X = torch.tensor([[ch]]).cuda()\n",
    "            \n",
    "            Y, hidden = net(X, hidden)\n",
    "            Y = F.softmax(Y, 1)\n",
    "\n",
    "            values, indices = Y[0].topk(k)\n",
    "            values, indices = values.cpu().numpy(), indices.cpu().numpy()\n",
    "            p = values / values.sum()\n",
    "            ch = np.random.choice(indices, p=p)\n",
    "\n",
    "    sentence = \"\".join([idx2char[idx] for idx in sentence])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (embedding): Embedding(82, 128)\n",
      "  (rnn): LSTM(128, 128, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=82, bias=True)\n",
      ")\n",
      "Epoch: 0, Iter: 0, Loss: 4.427\n",
      "Epoch: 0, Iter: 1000, Loss: 2.646\n",
      "Epoch: 0, Iter: 2000, Loss: 2.489\n",
      "Epoch: 0, Iter: 3000, Loss: 2.286\n",
      "Epoch: 0, Iter: 4000, Loss: 2.112\n",
      "Epoch: 0, Iter: 5000, Loss: 2.065\n",
      "Epoch: 0, Iter: 6000, Loss: 2.157\n",
      "Epoch: 0, Iter: 7000, Loss: 2.047\n",
      "Epoch: 0, Iter: 8000, Loss: 1.941\n",
      "Epoch: 0, Iter: 9000, Loss: 1.945\n",
      "Epoch: 1, Iter: 0, Loss: 1.899\n",
      "Epoch: 1, Iter: 1000, Loss: 1.803\n",
      "Epoch: 1, Iter: 2000, Loss: 1.859\n",
      "Epoch: 1, Iter: 3000, Loss: 1.851\n",
      "Epoch: 1, Iter: 4000, Loss: 1.748\n",
      "Epoch: 1, Iter: 5000, Loss: 1.768\n",
      "Epoch: 1, Iter: 6000, Loss: 1.775\n",
      "Epoch: 1, Iter: 7000, Loss: 1.793\n",
      "Epoch: 1, Iter: 8000, Loss: 1.688\n",
      "Epoch: 1, Iter: 9000, Loss: 1.744\n",
      "Epoch: 2, Iter: 0, Loss: 1.717\n",
      "Epoch: 2, Iter: 1000, Loss: 1.654\n",
      "Epoch: 2, Iter: 2000, Loss: 1.666\n",
      "Epoch: 2, Iter: 3000, Loss: 1.687\n",
      "Epoch: 2, Iter: 4000, Loss: 1.607\n",
      "Epoch: 2, Iter: 5000, Loss: 1.642\n",
      "Epoch: 2, Iter: 6000, Loss: 1.604\n",
      "Epoch: 2, Iter: 7000, Loss: 1.668\n",
      "Epoch: 2, Iter: 8000, Loss: 1.555\n",
      "Epoch: 2, Iter: 9000, Loss: 1.637\n",
      "Epoch: 3, Iter: 0, Loss: 1.614\n",
      "Epoch: 3, Iter: 1000, Loss: 1.580\n",
      "Epoch: 3, Iter: 2000, Loss: 1.555\n",
      "Epoch: 3, Iter: 3000, Loss: 1.587\n",
      "Epoch: 3, Iter: 4000, Loss: 1.524\n",
      "Epoch: 3, Iter: 5000, Loss: 1.564\n",
      "Epoch: 3, Iter: 6000, Loss: 1.509\n",
      "Epoch: 3, Iter: 7000, Loss: 1.584\n",
      "Epoch: 3, Iter: 8000, Loss: 1.464\n",
      "Epoch: 3, Iter: 9000, Loss: 1.567\n",
      "Epoch: 4, Iter: 0, Loss: 1.546\n",
      "Epoch: 4, Iter: 1000, Loss: 1.531\n",
      "Epoch: 4, Iter: 2000, Loss: 1.481\n",
      "Epoch: 4, Iter: 3000, Loss: 1.516\n",
      "Epoch: 4, Iter: 4000, Loss: 1.465\n",
      "Epoch: 4, Iter: 5000, Loss: 1.512\n",
      "Epoch: 4, Iter: 6000, Loss: 1.448\n",
      "Epoch: 4, Iter: 7000, Loss: 1.523\n",
      "Epoch: 4, Iter: 8000, Loss: 1.399\n",
      "Epoch: 4, Iter: 9000, Loss: 1.517\n",
      "&gedand, glad it,\n",
      "              Why that than find\n",
      "    A ponces, make stance sweer th is sturle\n",
      "    Thou? Srus, shall me to this dueficate\n",
      "    I pures muck hard. I dom then this, thire,\n",
      "    Olf her, nair'd, with bed the gaight!\n",
      "    Is'd his love ipon most were all stick\n",
      "    Antorn of his as to that I me ston\n",
      "    I'ed firng mine our brow and mone\n",
      "    Hath with thee bond when a sill in bove, misher\n",
      "    I chunknost orn wich me bught the, shall pide\n",
      "    Whith there.\n",
      "    Home tatks me old that better\n",
      "    If shall be been had by your have\n",
      "    Ind so be speep he make the deed servant'd their pleap\n",
      "    I again ale anblorys.\n",
      "  CHMLELIN. A lond there that!\n",
      "  CILANAIUS. A miston of th' word the charder.\n",
      "    This a my wourds mades, thereen of chee.\n",
      "    Whom is shome! I prosest, be the sors, sir,\n",
      "    If I'd plastield!\n",
      "  A Volding, the perv'd hell.\n",
      "  BELIARIUS. If, she love in then will maste to-maning\n",
      "    And to honour this not with your bestenges.\n",
      "    O should be not should breading.\n",
      "  SECON TYMA\n"
     ]
    }
   ],
   "source": [
    "rnn_type = \"LSTM\"\n",
    "net = train(num_epochs=5, rnn_type=rnn_type)\n",
    "text = sample(net, rnn_type=rnn_type, len_seq=1000, k=10)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (embedding): Embedding(82, 128)\n",
      "  (rnn): GRU(128, 128, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=82, bias=True)\n",
      ")\n",
      "Epoch: 0, Iter: 0, Loss: 4.426\n",
      "Epoch: 0, Iter: 1000, Loss: 2.290\n",
      "Epoch: 0, Iter: 2000, Loss: 2.256\n",
      "Epoch: 0, Iter: 3000, Loss: 2.117\n",
      "Epoch: 0, Iter: 4000, Loss: 1.945\n",
      "Epoch: 0, Iter: 5000, Loss: 1.916\n",
      "Epoch: 0, Iter: 6000, Loss: 1.984\n",
      "Epoch: 0, Iter: 7000, Loss: 1.923\n",
      "Epoch: 0, Iter: 8000, Loss: 1.815\n",
      "Epoch: 0, Iter: 9000, Loss: 1.823\n",
      "Epoch: 1, Iter: 0, Loss: 1.772\n",
      "Epoch: 1, Iter: 1000, Loss: 1.686\n",
      "Epoch: 1, Iter: 2000, Loss: 1.746\n",
      "Epoch: 1, Iter: 3000, Loss: 1.736\n",
      "Epoch: 1, Iter: 4000, Loss: 1.629\n",
      "Epoch: 1, Iter: 5000, Loss: 1.673\n",
      "Epoch: 1, Iter: 6000, Loss: 1.653\n",
      "Epoch: 1, Iter: 7000, Loss: 1.699\n",
      "Epoch: 1, Iter: 8000, Loss: 1.590\n",
      "Epoch: 1, Iter: 9000, Loss: 1.638\n",
      "Epoch: 2, Iter: 0, Loss: 1.605\n",
      "Epoch: 2, Iter: 1000, Loss: 1.558\n",
      "Epoch: 2, Iter: 2000, Loss: 1.578\n",
      "Epoch: 2, Iter: 3000, Loss: 1.585\n",
      "Epoch: 2, Iter: 4000, Loss: 1.507\n",
      "Epoch: 2, Iter: 5000, Loss: 1.558\n",
      "Epoch: 2, Iter: 6000, Loss: 1.508\n",
      "Epoch: 2, Iter: 7000, Loss: 1.581\n",
      "Epoch: 2, Iter: 8000, Loss: 1.470\n",
      "Epoch: 2, Iter: 9000, Loss: 1.543\n",
      "Epoch: 3, Iter: 0, Loss: 1.515\n",
      "Epoch: 3, Iter: 1000, Loss: 1.495\n",
      "Epoch: 3, Iter: 2000, Loss: 1.481\n",
      "Epoch: 3, Iter: 3000, Loss: 1.495\n",
      "Epoch: 3, Iter: 4000, Loss: 1.433\n",
      "Epoch: 3, Iter: 5000, Loss: 1.489\n",
      "Epoch: 3, Iter: 6000, Loss: 1.429\n",
      "Epoch: 3, Iter: 7000, Loss: 1.503\n",
      "Epoch: 3, Iter: 8000, Loss: 1.390\n",
      "Epoch: 3, Iter: 9000, Loss: 1.484\n",
      "Epoch: 4, Iter: 0, Loss: 1.458\n",
      "Epoch: 4, Iter: 1000, Loss: 1.455\n",
      "Epoch: 4, Iter: 2000, Loss: 1.417\n",
      "Epoch: 4, Iter: 3000, Loss: 1.432\n",
      "Epoch: 4, Iter: 4000, Loss: 1.383\n",
      "Epoch: 4, Iter: 5000, Loss: 1.444\n",
      "Epoch: 4, Iter: 6000, Loss: 1.378\n",
      "Epoch: 4, Iter: 7000, Loss: 1.447\n",
      "Epoch: 4, Iter: 8000, Loss: 1.331\n",
      "Epoch: 4, Iter: 9000, Loss: 1.444\n",
      "ANTH, LACIABER.               [Slements, his most, I\n",
      "    I drack, but I stands, offerve throne,\n",
      "    Thy beat was the well with as her temby; than that seek\n",
      "    To call as a was not.\n",
      "  Comunt'ce mother care my show. If he likes our face;\n",
      "    Yaurth my sind'd thought boor, my great have son.\n",
      "  PISANIO. By these flemituly.\n",
      "  GUIDERIUS. Nor her seap strange\n",
      "    Os thy lokes her; I sake him she\n",
      "    Bruther, and she must, what which speak,\n",
      "    As I that saith are make!\n",
      "    Yet when it ours her fealth of gone, the lief;\n",
      "    To be pyal that her comelus moture; most stom the misfect\n",
      "    The strong of lose and freedy, since,\n",
      "    Some to morrons of the pates of lord, and made\n",
      "\n",
      "    Mettrer well was all, which me though th' might\n",
      "    The horn of that- his cleator.\n",
      "\n",
      "                     Enter ORLIRIOM, and CALELIUS\n",
      "\n",
      "    And with her by me, sir,\n",
      "    Which I do not with my saight with mistress time\n",
      "    The begeing agaals thingtas, so prieful togen\n",
      "    Thou admands man she pelpose; no feed and begot\n",
      " \n"
     ]
    }
   ],
   "source": [
    "rnn_type = \"GRU\"\n",
    "net = train(num_epochs=5, rnn_type=rnn_type)\n",
    "text = sample(net, rnn_type=rnn_type, len_seq=1000, k=10)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (embedding): Embedding(82, 128)\n",
      "  (rnn): RNN(128, 128, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=82, bias=True)\n",
      ")\n",
      "Epoch: 0, Iter: 0, Loss: 4.400\n",
      "Epoch: 0, Iter: 1000, Loss: 2.257\n",
      "Epoch: 0, Iter: 2000, Loss: 2.258\n",
      "Epoch: 0, Iter: 3000, Loss: 2.118\n",
      "Epoch: 0, Iter: 4000, Loss: 1.963\n",
      "Epoch: 0, Iter: 5000, Loss: 1.927\n",
      "Epoch: 0, Iter: 6000, Loss: 1.998\n",
      "Epoch: 0, Iter: 7000, Loss: 1.951\n",
      "Epoch: 0, Iter: 8000, Loss: 1.852\n",
      "Epoch: 0, Iter: 9000, Loss: 1.858\n",
      "Epoch: 1, Iter: 0, Loss: 1.807\n",
      "Epoch: 1, Iter: 1000, Loss: 1.734\n",
      "Epoch: 1, Iter: 2000, Loss: 1.795\n",
      "Epoch: 1, Iter: 3000, Loss: 1.792\n",
      "Epoch: 1, Iter: 4000, Loss: 1.695\n",
      "Epoch: 1, Iter: 5000, Loss: 1.707\n",
      "Epoch: 1, Iter: 6000, Loss: 1.707\n",
      "Epoch: 1, Iter: 7000, Loss: 1.767\n",
      "Epoch: 1, Iter: 8000, Loss: 1.665\n",
      "Epoch: 1, Iter: 9000, Loss: 1.703\n",
      "Epoch: 2, Iter: 0, Loss: 1.667\n",
      "Epoch: 2, Iter: 1000, Loss: 1.633\n",
      "Epoch: 2, Iter: 2000, Loss: 1.656\n",
      "Epoch: 2, Iter: 3000, Loss: 1.667\n",
      "Epoch: 2, Iter: 4000, Loss: 1.589\n",
      "Epoch: 2, Iter: 5000, Loss: 1.618\n",
      "Epoch: 2, Iter: 6000, Loss: 1.592\n",
      "Epoch: 2, Iter: 7000, Loss: 1.668\n",
      "Epoch: 2, Iter: 8000, Loss: 1.563\n",
      "Epoch: 2, Iter: 9000, Loss: 1.622\n",
      "Epoch: 3, Iter: 0, Loss: 1.592\n",
      "Epoch: 3, Iter: 1000, Loss: 1.579\n",
      "Epoch: 3, Iter: 2000, Loss: 1.574\n",
      "Epoch: 3, Iter: 3000, Loss: 1.590\n",
      "Epoch: 3, Iter: 4000, Loss: 1.527\n",
      "Epoch: 3, Iter: 5000, Loss: 1.565\n",
      "Epoch: 3, Iter: 6000, Loss: 1.524\n",
      "Epoch: 3, Iter: 7000, Loss: 1.599\n",
      "Epoch: 3, Iter: 8000, Loss: 1.494\n",
      "Epoch: 3, Iter: 9000, Loss: 1.571\n",
      "Epoch: 4, Iter: 0, Loss: 1.543\n",
      "Epoch: 4, Iter: 1000, Loss: 1.543\n",
      "Epoch: 4, Iter: 2000, Loss: 1.517\n",
      "Epoch: 4, Iter: 3000, Loss: 1.538\n",
      "Epoch: 4, Iter: 4000, Loss: 1.483\n",
      "Epoch: 4, Iter: 5000, Loss: 1.528\n",
      "Epoch: 4, Iter: 6000, Loss: 1.477\n",
      "Epoch: 4, Iter: 7000, Loss: 1.548\n",
      "Epoch: 4, Iter: 8000, Loss: 1.443\n",
      "Epoch: 4, Iter: 9000, Loss: 1.535\n",
      "ENG. If, as necks her from their did, and things he be it.\n",
      "    Beat telend what his ching the pervon first antreans be prisons!\n",
      "  MENENIUS. Go shurt noble speed; who shall honour looks, and spoket, time it this'd; and honess what a most war for my told.                                                                                          [Asword outh\n",
      "    And sure the bound\n",
      "    Speok hank him; of your gengicle,\n",
      "    When strange this prince.\n",
      "  POSTHUMUS] O POSTHIMUS. By might\n",
      "    That the breatues worthing theirs to this creat his trust is beliver my be time, what that's midery, fill\n",
      "    I hound than was serve, my poor did a two he shalk them. Made me mother show'd to me him belear me to my take in herce.\n",
      "    And think in thret man thye me whither?                      \n",
      "    I dried; must he man as your mencher'd my sod\n",
      "    What stald, and news. Think himsern for this honourn\n",
      "    The mook brees in\n",
      "    If the tonest,\n",
      "    Thou more,\n",
      "    To anour dissed some arantand to give balosa. If t\n"
     ]
    }
   ],
   "source": [
    "rnn_type = \"RNN\"\n",
    "net = train(num_epochs=5, rnn_type=rnn_type)\n",
    "text = sample(net, rnn_type=rnn_type, len_seq=1000, k=10)\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
